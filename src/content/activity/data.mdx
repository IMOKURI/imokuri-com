---
title: Data
slug: data
description: "調べたり、集めたりしたデータです。"
---

調べたり、集めたりしたデータです。

## LLM KV Cache Sizing

updated: 2025-05-14


[この記事](/blog/2025/05/llm-kv-cache-size/)に従って、LLMのKV CacheのサイズとActivation Peak Memoryを試算しました。
あくまで机上の計算なので、実際のサイズとは異なる場合があります。


### google/gemma-3-12b-it

| 記号                           | 値               |
|--------------------------------|------------------|
| 2                              | 2                |
| B                              | 1 (とする)       |
| bytes/param                    | 2 (16 bit)       |
| n<sub>layers</sub>             | 48               |
| n<sub>kv_attention_heads</sub> | 8                |
| d<sub>attention_heads</sub>    | 3840 // 16 = 240 |
| context_length                 | 32k (とする)     |
| KV Cache サイズ                | 11GB             |

| 記号                   | 値           | 途中計算     |
|------------------------|--------------|--------------|
| hidden_size            | 3840         | x 18 = 69120 |
| intermediate_size      | 15360        | x 4 = 61440  |
| context_length         | 32k (とする) |              |
| Activation Peak Memory | 3.9GB        |              |

[google/gemma-3-12b-it (config.json)](https://huggingface.co/google/gemma-3-12b-it/blob/main/config.json)


### google/gemma-3-27b-it

| 記号                           | 値               |
|--------------------------------|------------------|
| 2                              | 2                |
| B                              | 1 (とする)       |
| bytes/param                    | 2 (16 bit)       |
| n<sub>layers</sub>             | 62               |
| n<sub>kv_attention_heads</sub> | 16               |
| d<sub>attention_heads</sub>    | 5376 // 32 = 168 |
| context_length                 | 32k (とする)     |
| KV Cache サイズ                | 19.9GB           |

| 記号                   | 値           | 途中計算     |
|------------------------|--------------|--------------|
| hidden_size            | 5376         | x 18 = 96768 |
| intermediate_size      | 21504        | x 4 = 86016  |
| context_length         | 32k (とする) |              |
| Activation Peak Memory | 5.4GB        |              |

[google/gemma-3-27b-it (config.json)](https://huggingface.co/google/gemma-3-27b-it/blob/main/config.json)


### Qwen/Qwen3-32B

| 記号                           | 値              |
|--------------------------------|-----------------|
| 2                              | 2               |
| B                              | 1 (とする)      |
| bytes/param                    | 2 (16 bit)      |
| n<sub>layers</sub>             | 64              |
| n<sub>kv_attention_heads</sub> | 8               |
| d<sub>attention_heads</sub>    | 5120 // 64 = 80 |
| context_length                 | 32k (とする)    |
| KV Cache サイズ                | 4.9GB           |

| 記号                   | 値           | 途中計算     |
|------------------------|--------------|--------------|
| hidden_size            | 5120         | x 18 = 92160 |
| intermediate_size      | 25600        | x 4 = 102400 |
| context_length         | 32k (とする) |              |
| Activation Peak Memory | 5.8GB        |              |

[Qwen/Qwen3-32B (config.json)](https://huggingface.co/Qwen/Qwen3-32B/blob/main/config.json)


### meta-llama/Llama-4-Scout-17B-16E-Instruct

| 記号                           | 値               |
|--------------------------------|------------------|
| 2                              | 2                |
| B                              | 1 (とする)       |
| bytes/param                    | 2 (16 bit)       |
| n<sub>layers</sub>             | 48               |
| n<sub>kv_attention_heads</sub> | 8                |
| d<sub>attention_heads</sub>    | 5120 // 40 = 128 |
| context_length                 | 32k (とする)     |
| KV Cache サイズ                | 5.9GB            |

| 記号                   | 値           | 途中計算     |
|------------------------|--------------|--------------|
| hidden_size            | 5120         | x 18 = 92160 |
| intermediate_size      | 16384        | x 4 = 65536  |
| context_length         | 32k (とする) |              |
| Activation Peak Memory | 4.7GB        |              |

[meta-llama/Llama-4-Scout-17B-16E-Instruct (config.json)](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct/blob/main/config.json)
